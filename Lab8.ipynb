{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9180e400",
   "metadata": {},
   "source": [
    "## Object Following (50 pts)\n",
    "\n",
    "In this notebook we'll show how you can follow an object with the crazyflie!  We'll use a pre-trained neural network that was trained on the [COCO dataset](http://cocodataset.org) to detect 90 different common objects.  These include\n",
    "\n",
    "* Person (index 0)\n",
    "* Cup (index 47)\n",
    "\n",
    "and many others (you can check [this file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) for a full list of class indices). We use the MobileNet SSD (Single Shot Detector) trained on the COCO dataset. SSD models are often faster than other detection models and the MobileNet backbone is less computationally intensive, so this will help for real-time execution! The model is sourced from the [TensorFlow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection),\n",
    "which provides utilities for training object detectors for custom tasks also!\n",
    "\n",
    "We won't run through all of the training and optimization steps in this notebook though. The goal here is to demonstrate what one can do with neural networks. \n",
    "\n",
    "Anyways, let's get started!  First, we will load the pre-trained network. Make sure to have the Lab8_Supplement directory downloaded. Also download the model and place in the Lab8_Supplement directory [https://drive.google.com/file/d/1vIS9XySf5kdmVqPCtCpHG_-FL6RB8oOP/view](https://drive.google.com/file/d/1vIS9XySf5kdmVqPCtCpHG_-FL6RB8oOP/view)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e35757",
   "metadata": {},
   "source": [
    "### Compute detections on single camera image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e379cb92",
   "metadata": {},
   "source": [
    "For this lab, we will be using OpenCV's DNN module which provides us with functionalities for deep learning inference. You can read more about how we are using it for [object detection](https://learnopencv.com/deep-learning-with-opencvs-dnn-module-a-definitive-guide/). Specifically, we can load in the MobileNet SSD network that was trained using the Tensorflow framework. OpenCV's DNN module allows for multi-framework use (e.g., PyTorch and Caffe).\n",
    "\n",
    "First, we load in the COCO class names (e.g., person, potted plant, etc.), assign colors to the classes (this is useful for visualizing bounding boxes), and load the weights of the pre-trained neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11388b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# load the COCO class names\n",
    "with open('Lab8_Supplement/object_detection_classes_coco.txt', 'r') as f:\n",
    "    class_names = f.read().split('\\n')\n",
    "    \n",
    "# get a different color array for each of the classes\n",
    "COLORS = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
    "\n",
    "# load the DNN model\n",
    "model = cv2.dnn.readNet(model='Lab8_Supplement/frozen_inference_graph.pb',\n",
    "                        config='Lab8_Supplement/ssd_mobilenet_v2_coco_2018_03_29.pbtxt.txt', \n",
    "                        framework='TensorFlow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e4523",
   "metadata": {},
   "source": [
    "Now we will prepare an image for object detection with our model. `blobFromImage()` prepares the image into the correct format for our model. Specifically, we resize our input image to 300x300 and normalize the RGB channels with the mean parameter. Then we forward propagate the image through the model to obtain the detections. Each detection is of the form ( _, class_id, confidence, box_x, box_y, box_width, box_height) where box_x, box_y, box_width, box_height provide information for creating the bounding box of around the detected object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94efb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the image from disk\n",
    "image = cv2.imread('Lab8_Supplement/Lab8_image.jpg')\n",
    "image_height, image_width, _ = image.shape\n",
    "\n",
    "# create blob from image\n",
    "blob = cv2.dnn.blobFromImage(image=image, size=(300, 300), mean=(104, 117, 123), \n",
    "                             swapRB=True)\n",
    "\n",
    "# create blob from image\n",
    "model.setInput(blob)\n",
    "\n",
    "# forward pass through the model to carry out the detection\n",
    "detections = model.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad334d",
   "metadata": {},
   "source": [
    "Next we visualize the detections. You should see a bounding box, classification, and confidence value appear around each COCO object (potted plant and cup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7bea59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop over each of the detection\n",
    "for detection in detections[0, 0, :, :]:\n",
    "    # extract the confidence of the detection\n",
    "    confidence = detection[2]\n",
    "    # draw bounding boxes only if the detection confidence is above...\n",
    "    # ... a certain threshold, else skip\n",
    "    if confidence > .4:\n",
    "        # get the class id\n",
    "        class_id = detection[1]\n",
    "        # map the class id to the class\n",
    "        class_name = class_names[int(class_id)-1]\n",
    "        color = COLORS[int(class_id)]\n",
    "        # get the bounding box coordinates\n",
    "        box_x = detection[3] * image_width\n",
    "        box_y = detection[4] * image_height\n",
    "        # get the bounding box width and height\n",
    "        box_width = detection[5] * image_width\n",
    "        box_height = detection[6] * image_height\n",
    "        # draw a rectangle around each detected object\n",
    "        cv2.rectangle(image, (int(box_x), int(box_y)), (int(box_width), int(box_height)), color, thickness=1)\n",
    "        # put the FPS text on top of the frame\n",
    "        text = class_name + ' ' + '%.2f' % (confidence)\n",
    "        cv2.putText(image, text, (int(box_x), int(box_y - 5)), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, color, 1)\n",
    "\n",
    "while(True):\n",
    "    cv2.imshow('image', image)\n",
    "    \n",
    "    # Hit q to quit.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite('Lab8_Supplement/image_result.jpg', image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c85278",
   "metadata": {},
   "source": [
    "To print just the first object detected in the example image, we could call the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2b2630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0000000e+00 6.2000000e+01 3.2870751e-02 8.9506602e-01 4.6672899e-01\n",
      " 9.8932624e-01 7.9487997e-01]\n"
     ]
    }
   ],
   "source": [
    "object_number = 85\n",
    "print(detections[0, 0, object_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76512e87",
   "metadata": {},
   "source": [
    "### Compute detections on a live video feed\n",
    "\n",
    "The following cell will perform the same object detection and labeling on a live feed from your CrazyFlie camera! Note that the drone will not fly, you are simply using the camera. This should give you a sense of appropriate distances for detection, as well as the confidence for the detection of different objects from the Coco dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5196c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may open your webcam instead of the CrazyFlie camera! If so, try\n",
    "# a different small, positive integer, e.g. 1, 2, 3.\n",
    "camera = 1\n",
    "cap = cv2.VideoCapture(camera)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    image_height, image_width, _ = frame.shape\n",
    "\n",
    "    # create blob from image\n",
    "    blob = cv2.dnn.blobFromImage(image=frame, size=(300, 300), mean=(104, 117, 123), \n",
    "                                 swapRB=True)\n",
    "\n",
    "    # create blob from image\n",
    "    model.setInput(blob)\n",
    "\n",
    "    # forward pass through the model to carry out the detection\n",
    "    detections = model.forward()\n",
    "\n",
    "    # loop over each of the detection\n",
    "    for detection in detections[0, 0, :, :]:\n",
    "        # extract the confidence of the detection\n",
    "        confidence = detection[2]\n",
    "        # draw bounding boxes only if the detection confidence is above...\n",
    "        # ... a certain threshold, else skip\n",
    "        if confidence > .4:\n",
    "            # get the class id\n",
    "            class_id = detection[1]\n",
    "            # map the class id to the class\n",
    "            class_name = class_names[int(class_id)-1]\n",
    "            color = COLORS[int(class_id)]\n",
    "            # get the bounding box coordinates\n",
    "            box_x = detection[3] * image_width\n",
    "            box_y = detection[4] * image_height\n",
    "            # get the bounding box width and height\n",
    "            box_width = detection[5] * image_width\n",
    "            box_height = detection[6] * image_height\n",
    "            # draw a rectangle around each detected object\n",
    "            cv2.rectangle(frame, (int(box_x), int(box_y)), (int(box_width), int(box_height)), color, thickness=1)\n",
    "            # put the FPS text on top of the frame\n",
    "            text = class_name + ' ' + '%.2f' % (confidence)\n",
    "            cv2.putText(frame, text, (int(box_x), int(box_y - 5)), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, color, 1)\n",
    "\n",
    "    # Compute\n",
    "    cv2.imshow('frame', frame)    \n",
    "\n",
    "    # Hit q to quit.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4693eff",
   "metadata": {},
   "source": [
    "### Control robot to follow central object\n",
    "\n",
    "Now we want our robot to follow an object of the specified category (e.g., person, etc.).  To do this we'll do the following\n",
    "\n",
    "1.  Detect objects matching the specified class\n",
    "2.  Select object closest to center of camera's field of vision; this is the 'target' object\n",
    "3.  Control the robot towards target object; otherwise hover\n",
    "\n",
    "We'll also create a controller that will use the distance between the target object and the center of the robot's field of view to follow the object as well as use the bounding box size to determine when to stop. \n",
    "\n",
    "First, let's define some functions that will process the images from the crazyflie. \n",
    "\n",
    "### Task 1 (10 pts) ###\n",
    "\n",
    "Fill in the function \"closest_detection\" below. This should find the detected object that is closest to the center of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d347843b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    center_x = (detection[3] + detection[5]) / 2.0 - 0.5\n",
    "    center_y = (detection[4] + detection[6]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "\n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"TODO: Find the detection closest to the image center\"\"\"\n",
    "    # Loop through and find the detection that is closest to the image center\n",
    "    # You can use the detection_center function above to find the center of the detected object\n",
    "    # Note that the origin (i.e., (x,y) = (0,0)) corresponds to the center of the image. So you can\n",
    "    # use the \"norm\" function above to find the detection that is closest to the center.\n",
    "    # Return the det that corresponds to the closest detection to the image center.\n",
    "    # If nothing is detected, return None.\n",
    "\n",
    "    if len(detections) == 0:\n",
    "        return None\n",
    "\n",
    "    dists = np.zeros(len(detections))\n",
    "    \n",
    "    for i in range(len(detections)):\n",
    "        \n",
    "        center = detection_center(detections[i])\n",
    "        dist = norm(center)\n",
    "        dists[i]= dist\n",
    "\n",
    "    mindex = dists.argmin()\n",
    "\n",
    "    return detections[mindex]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f4381",
   "metadata": {},
   "source": [
    "Great, now let's get ready to control the crazyflie to follow an object! Below are a few functions to help move the crazyflie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1723ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cflib.crtp\n",
    "from cflib.crazyflie import Crazyflie\n",
    "from cflib.crazyflie.log import LogConfig\n",
    "from cflib.crazyflie.syncCrazyflie import SyncCrazyflie\n",
    "from cflib.crazyflie.syncLogger import SyncLogger\n",
    "from cflib.positioning.position_hl_commander import PositionHlCommander\n",
    "from cflib.positioning.motion_commander import MotionCommander\n",
    "\n",
    "\n",
    "def wait_for_position_estimator(scf):\n",
    "    print('Waiting for estimator to find position...')\n",
    "\n",
    "    log_config = LogConfig(name='Kalman Variance', period_in_ms=500)\n",
    "    log_config.add_variable('kalman.varPX', 'float')\n",
    "    log_config.add_variable('kalman.varPY', 'float')\n",
    "    log_config.add_variable('kalman.varPZ', 'float')\n",
    "\n",
    "    var_y_history = [1000] * 10\n",
    "    var_x_history = [1000] * 10\n",
    "    var_z_history = [1000] * 10\n",
    "\n",
    "    threshold = 0.001\n",
    "    with SyncLogger(scf, log_config) as logger:\n",
    "        for log_entry in logger:\n",
    "            data = log_entry[1]\n",
    "\n",
    "            var_x_history.append(data['kalman.varPX'])\n",
    "            var_x_history.pop(0)\n",
    "            var_y_history.append(data['kalman.varPY'])\n",
    "            var_y_history.pop(0)\n",
    "            var_z_history.append(data['kalman.varPZ'])\n",
    "            var_z_history.pop(0)\n",
    "\n",
    "            min_x = min(var_x_history)\n",
    "            max_x = max(var_x_history)\n",
    "            min_y = min(var_y_history)\n",
    "            max_y = max(var_y_history)\n",
    "            min_z = min(var_z_history)\n",
    "            max_z = max(var_z_history)\n",
    "\n",
    "            print(\"{} {} {}\".\n",
    "                format(max_x - min_x, max_y - min_y, max_z - min_z))\n",
    "\n",
    "            if (max_x - min_x) < threshold and (\n",
    "                    max_y - min_y) < threshold and (\n",
    "                    max_z - min_z) < threshold:\n",
    "                break\n",
    "\n",
    "def set_PID_controller(cf):\n",
    "    # Set the PID Controller:\n",
    "    print('Initializing PID Controller')\n",
    "    cf.param.set_value('stabilizer.controller', '1')\n",
    "    cf.param.set_value('kalman.resetEstimation', '1')\n",
    "    time.sleep(0.1)\n",
    "    cf.param.set_value('kalman.resetEstimation', '0')\n",
    "    \n",
    "    wait_for_position_estimator(cf)\n",
    "    time.sleep(0.1)    \n",
    "    return\n",
    "\n",
    "# Ascend and hover:\n",
    "def ascend_and_hover(cf):\n",
    "    # Ascend:\n",
    "    for y in range(10):\n",
    "        cf.commander.send_hover_setpoint(0, 0, 0, y / 10)\n",
    "        time.sleep(0.1)\n",
    "    # Hover at 1 meter:\n",
    "    for _ in range(20):\n",
    "        cf.commander.send_hover_setpoint(0, 0, 0, 1)\n",
    "        time.sleep(0.1)\n",
    "    return\n",
    "\n",
    "def hover(cf):\n",
    "    print('Hovering:')\n",
    "    # Hover at 1 meter:\n",
    "    for _ in range(30):\n",
    "        cf.commander.send_hover_setpoint(0, 0, 0, 1)\n",
    "        time.sleep(0.1)\n",
    "    return\n",
    "    \n",
    "# Hover, descend, and stop all motion:\n",
    "def hover_and_descend(cf):\n",
    "    # Hover at 1 meter:\n",
    "    for _ in range(30):\n",
    "        cf.commander.send_hover_setpoint(0, 0, 0, 1)\n",
    "        time.sleep(0.1)\n",
    "    # Descend:\n",
    "    for y in range(10):\n",
    "        cf.commander.send_hover_setpoint(0, 0, 0, (10 - y) / 10)\n",
    "        time.sleep(0.1)\n",
    "    # Stop all motion:\n",
    "    for i in range(10):\n",
    "        cf.commander.send_stop_setpoint()\n",
    "        time.sleep(0.1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c3203",
   "metadata": {},
   "source": [
    "### Task 2 (20 pts) ###\n",
    "\n",
    "Fill in the controller below that says \"TODO\" to make the crazyflie follow the object. The controller should use the inputs to keep the detected target in the center of its view as well determine when to stop (send True flag) so that the crazyflie stops and lands before crashing into the tracked object. (Note: the execution code implements the actual stopping using the flag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d32a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def controller(cf, box_x, box_y, box_width, box_height, x_cur, y_cur):\n",
    "    \"\"\"\n",
    "    \n",
    "    cf: crazyflie instance\n",
    "    box_x: x coordinate of the center of the bounding box in the image\n",
    "    box_y: y coordinate of the center of the bounding box in the image\n",
    "    box_width: width of the bounding box in the image\n",
    "    box_height: height of the bounding box in the image\n",
    "    x_cur: current x position\n",
    "    y_cur: current y position\n",
    "    \n",
    "    Return True to indicate that the drone is close to the target and thus exit the loop to stop and descend, new x, new y\n",
    "    Return False to indicate continuing to follow the target, new x, new y.\n",
    "    \n",
    "    \"\"\"\n",
    "    #### TO DO: Fill below ####\n",
    "    # Exit condition/method using size of the bounding box\n",
    "    # We experimentally found 0.993 to be the value of the box height for a person\n",
    "    # at a reasonably close distance to stop using the live-video feed cell \n",
    "    if box_height > 0.993:\n",
    "        print(\"Too close!\")\n",
    "        return True, x_cur, y_cur\n",
    "    \n",
    "    #### TO DO: Fill below ####\n",
    "    # Determine the x and y velocity\n",
    "    error_x = box_x / np.abs(box_x)\n",
    "        \n",
    "    x_command = x_cur + 0.2\n",
    "    \n",
    "    y_command = y_cur - box_x\n",
    "    \n",
    "    # Set velocity\n",
    "    cf.commander.send_position_setpoint(x_command, y_command, 1, 0) # Do not edit this line\n",
    "\n",
    "    return False, x_command, y_command\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde9a93",
   "metadata": {},
   "source": [
    "The following code will test your controller on the crazyflie. There are several parameters at the top that may be useful to change as indicated, otherwise do not modify the code. Please read the safety and submission instructions below before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3377304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning interfaces for Crazyflies...\n",
      "Crazyflies found:\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "radio://0/14/2M\n",
      "radio://0/16/2M\n",
      "Initializing PID Controller\n",
      "Waiting for estimator to find position...\n",
      "999.9999879754478 999.9999879700081 999.9998099823424\n",
      "999.9999882917873 999.9999882712445 999.9998131492612\n",
      "999.9999885775887 999.9999885655898 999.9998131662578\n",
      "999.9999885775887 999.9999885655898 999.9998131662578\n",
      "999.9999885775887 999.9999885655898 999.9998131662578\n",
      "999.9999885775887 999.9999885655898 999.9998145560239\n",
      "999.9999885775887 999.9999885655898 999.9998145560239\n",
      "999.9999885775887 999.9999885655898 999.9998145560239\n",
      "999.9999885775887 999.9999885655898 999.9998145560239\n",
      "1.7878201106213965e-06 1.8190294213127345e-06 3.688321157824248e-05\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "detection...tracking\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 123\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m det \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno detection...hovering\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m     \u001b[43mhover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# otherwise  move towards target\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetection...tracking\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36mhover\u001b[0;34m(cf)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m     77\u001b[0m     cf\u001b[38;5;241m.\u001b[39mcommander\u001b[38;5;241m.\u001b[39msend_hover_setpoint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# load the COCO class names\n",
    "with open('Lab8_Supplement/object_detection_classes_coco.txt', 'r') as f:\n",
    "    class_names = f.read().split('\\n')\n",
    "\n",
    "# get a different color array for each of the classes\n",
    "COLORS = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
    "\n",
    "# load the DNN model\n",
    "model = cv2.dnn.readNet(model='Lab8_Supplement/frozen_inference_graph.pb',\n",
    "                        config='Lab8_Supplement/ssd_mobilenet_v2_coco_2018_03_29.pbtxt.txt', \n",
    "                        framework='TensorFlow')\n",
    "\n",
    "# ************ Parameters that might be useful to change ************ \n",
    "# COCO label id that we want to track\n",
    "tracking_label = 85 # PERSON (1), CHAIR (62)\n",
    "\n",
    "# Set the URI the Crazyflie will connect to\n",
    "group_number = 14\n",
    "uri = f'radio://0/{group_number}/2M'\n",
    "\n",
    "# Possibly try 0, 1, 2 ...\n",
    "camera = 1\n",
    "\n",
    "# Confidence of detection\n",
    "confidence = 0.4\n",
    "\n",
    "# ******************************************************************\n",
    "\n",
    "# Initialize all the CrazyFlie drivers:\n",
    "cflib.crtp.init_drivers(enable_debug_driver=False)\n",
    "\n",
    "# Scan for Crazyflies in range of the antenna:\n",
    "print('Scanning interfaces for Crazyflies...')\n",
    "available = cflib.crtp.scan_interfaces()\n",
    "\n",
    "# List local CrazyFlie devices:\n",
    "print('Crazyflies found:')\n",
    "for i in available:\n",
    "    print(i[0])\n",
    "\n",
    "if len(available) == 0:\n",
    "    print('No Crazyflies found, cannot run example')\n",
    "else:\n",
    "    ## Ascend to hover; run the sequence; then descend from hover:\n",
    "    # Use the CrazyFlie corresponding to team number:\n",
    "    with SyncCrazyflie(uri, cf=Crazyflie(rw_cache='./cache')) as scf:\n",
    "        # Get the Crazyflie class instance:\n",
    "        cf = scf.cf\n",
    "\n",
    "        # Initialize and ascend:\n",
    "        t = time.time()\n",
    "        elapsed = time.time() - t\n",
    "        ascended_bool = 0\n",
    "\n",
    "        # capture the video\n",
    "        cap = cv2.VideoCapture(camera)\n",
    "        \n",
    "        # get the video frames' width and height\n",
    "        frame_width = int(cap.get(3))\n",
    "        frame_height = int(cap.get(4))\n",
    "\n",
    "        # flag indicating whether to exit the main loop and then descend\n",
    "        exit_loop = False\n",
    "\n",
    "        # Ascend and hover a bit\n",
    "        set_PID_controller(cf)\n",
    "        ascend_and_hover(cf)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        x_cur = 0\n",
    "        y_cur = 0\n",
    "        \n",
    "        # detect objects in each frame of the video\n",
    "        while cap.isOpened() and not exit_loop:\n",
    "            \n",
    "            # Try to read image\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                image = frame\n",
    "                image_height, image_width, _ = image.shape\n",
    "\n",
    "                # create blob from image\n",
    "                blob = cv2.dnn.blobFromImage(image=image, size=(300, 300), mean=(104, 117, 123), \n",
    "                                             swapRB=True)\n",
    "\n",
    "                # forward propagate image\n",
    "                model.setInput(blob)\n",
    "                detections = model.forward()\n",
    "\n",
    "                # select detections that match selected class label\n",
    "                matching_detections = [d for d in detections[0, 0] if d[1] == tracking_label]\n",
    "\n",
    "                # select confident detections\n",
    "                confident_detections = [d for d in matching_detections if d[2] > confidence]\n",
    "\n",
    "                # get detection closest to center of field of view and draw it\n",
    "                det = closest_detection(confident_detections) # This relies on the function you wrote above\n",
    "                \n",
    "                if det is not None:\n",
    "                    # get the class id\n",
    "                    class_id = det[1]\n",
    "                    # map the class id to the class \n",
    "                    class_name = class_names[int(class_id)-1]\n",
    "                    color = COLORS[int(class_id)]\n",
    "                    # get the bounding box coordinates\n",
    "                    box_x = det[3] * image_width\n",
    "                    box_y = det[4] * image_height\n",
    "                    # get the bounding box width and height\n",
    "                    box_width = det[5] * image_width\n",
    "                    box_height = det[6] * image_height\n",
    "                    # draw a rectangle around each detected object\n",
    "                    cv2.rectangle(image, (int(box_x), int(box_y)), (int(box_width), int(box_height)), color, thickness=2)\n",
    "                    # put the class name text on the detected object\n",
    "                    cv2.putText(image, class_name, (int(box_x), int(box_y - 5)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "                # If nothing is detected, hover\n",
    "                if det is None:\n",
    "                    print('no detection...hovering')\n",
    "                    hover(cf)\n",
    "\n",
    "                # otherwise  move towards target\n",
    "                else:\n",
    "                    print('detection...tracking')\n",
    "                    _, _, _, box_x, box_y, box_width, box_height = det\n",
    "                    box_x, box_y = detection_center(det)\n",
    "                    exit_loop, x_cur, y_cur = controller(cf, box_x, box_y, box_width, box_height, x_cur, y_cur)\n",
    "\n",
    "                # Check image\n",
    "                cv2.imshow('image', image)\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                print('no image!!')\n",
    "                \n",
    "        cap.release()\n",
    "        \n",
    "        # Descend and stop all motion:\n",
    "        hover_and_descend(cf)\n",
    "        \n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d7c43",
   "metadata": {},
   "source": [
    "If the previous cell has an error or you lose connection with your drone, run the following cell and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc00e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82375b6",
   "metadata": {},
   "source": [
    "# Submission #\n",
    "\n",
    "Please submit to Gradescope \"HW8: Coding\" a zip including: this notebook Lab8 (30pts), a video (20pts see below), and Lab9 notebook (50pts).\n",
    "\n",
    "For the video, please submit the following:\n",
    "- (10 pts) A video (e.g., taken from your cellphone) showing the crazyflie following you (or any other person). The person should be moving such that it is clear the crazyflie is changing its tracking to follow the person. Read safety instructions below before trying! The crazyflie should stop and land when close to the person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553bf6b1",
   "metadata": {},
   "source": [
    "# Safety #\n",
    "\n",
    "As always, please wear your safety glasses when working with the crazyflie. \n",
    "\n",
    "Additionally, for human tracking, please stand OUTSIDE of the netted test space. The drone's camera is capable of detecting people standing behind the net. \n",
    "\n",
    "**NOTE**\n",
    "We tried multiple times outside of the net and the camera was unable to identify us so we tested inside the netted space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa32d3-a1e5-4667-b314-d8ad97fdf4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
